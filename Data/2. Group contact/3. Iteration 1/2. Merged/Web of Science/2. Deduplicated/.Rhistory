# prominent keywords.
gs_group_contact$title[2] # "intergroup contact", "prejudice toward immigrants", "individual conservative
# values", "cultural embeddedness", are prominent keywords.
gs_group_contact$title[3] # "countervailing contact", "ethnic diversity", "anti immigrant attitudes",
# "positive inter-ethnic contact, "negative inter-ethnic contact", are prominent keywords.
gs_group_contact$title[4] # "xenophobia", "inter-ethnic contact", are prominent keywords.
gs_group_contact$title[5] # "social contact", "prejudice", "discrimination", are prominent keywords.
gs_group_contact$title[6] # "extended contact", "direct contact", "group norms", "positive ethnic intergroup
# attitudes", are prominent keywords.
## Important abstract keywords.
gs_group_contact$abstract[1] # "ethnic diversity", "social cohesion", "positive intergroup contact", are
# prominent keywords.
gs_group_contact$abstract[2] # "individual conservative values", "cultural embeddedness", "contact with
# immigrants", "attitudes toward immigrants", "ethnic prejudice', "prejudice", are prominent keywords.
gs_group_contact$abstract[3] # "inter-ethnic contact", "contact-valence", "attitudes towards immigrants",
# "inter-group contact, "positive inter-group contact", "negative inter-group contact", "diverse
# communities", are prominent keywords.
gs_group_contact$abstract[4] # "xenophobic attitudes", "xenophobia", "inter-ethnic contact", "positive
# inter-ethnic contact", "negative inter-ethnic contact", are prominent keywords.
gs_group_contact$abstract[5] # "positive social contact", "social contact", "positive integroup social
# contact", "prejudice", "discrimination", are prominent keywords.
gs_group_contact$abstract[6] # "direct contact", "extended contact", "in-group norms", "out-group norms",
# "cross-ethnic friendships", are prominent keywords.
## Making a selection on whether a keyword is either a dependent or independent variable of interest, or is
## prominent in the title or abstract of the gold standard articles.
gs_grouped_terms <- list(
determinant = gs_all_keywords[c(8, 9, 11, 15, 25, 26, 33, 35, 37, 39, 40, 42, 43, 44, 50, 52, 53)],
outcome = c(gs_all_keywords[c(18, 21, 23, 28, 34, 51, 56, 57, 58, 59, 70)]))
gs_grouped_terms # Final set of naive keywords.
gs_group_contact$title
### Now repeat the previous procedure, but with keywords extracted from the naive search corpus.
### Clear the environment except for the gold standard articles and "naive_dedup" objects.
rm(list=setdiff(ls(), c("gs_grouped_terms", "gs_group_contact", "naive_dedup")))
### Start by checking the naive search corpus for provided keywords, and keywords in titles and abstracts.
## Keywords.l
length(naive_dedup$keywords) # Total number of documents is 2,043.
length(naive_dedup$keywords) - sum(is.na(naive_dedup$keywords)) # 2,041 of the articles list keywords.
## Titles and abstracts.
length(naive_dedup$title) - sum(is.na(naive_dedup$title)) # All of the articles list a title.
length(naive_dedup$abstract) - sum(is.na(naive_dedup$abstract)) # 1,982 of the articles list an abstract.
## Save .ris file of the merged file.
setwd("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/2. Naive search/2. Merged")
# EndNote and Zotero both indicate that seven duplicates remain in the .ris file. We furthermore remove three
# retracted papers: "Interprofessional learning in acute care: Developing a theoretical framework",
# "Bridging the Gap on Facebook: Assessing Intergroup Contact and Its Effects for Intergroup Relations", and
# "When contact changes minds: An experiment on transmission of support for gay equality". The second paper
# has two documents in the set, one on the paper itself, and one on the retraction. The other two have just
# one paper. The result is exported in the file "naive_dedup.ris" which is subsequently imported.
naive_dedup <- read_bibliography("naive_dedup.ris")
length(naive_dedup$title) # 2,043 documents. 11 documents removed.
### Clear the environment except for the gold standard articles and "naive_dedup" objects.
rm(list=setdiff(ls(), c("gs_grouped_terms", "gs_group_contact", "naive_dedup")))
### Start by checking the naive search corpus for provided keywords, and keywords in titles and abstracts.
## Keywords.l
length(naive_dedup$keywords) # Total number of documents is 2,043.
length(naive_dedup$keywords) - sum(is.na(naive_dedup$keywords)) # 2,041 of the articles list keywords.
## Titles and abstracts.
length(naive_dedup$title) - sum(is.na(naive_dedup$title)) # All of the articles list a title.
length(naive_dedup$abstract) - sum(is.na(naive_dedup$abstract)) # 1,982 of the articles list an abstract.
### The first step is to obtain the keywords listed in the articles. We extract all non-single keywords that
### are listed a minimum of twenty times. Note that the twenty number is arbitrary insofar that we need to
### strike a balance between retrieving enough but not too many keyword candidates. More specifically,
### Gusenbauer & Haddaway (2020) note that search terms of length 25 should allow reviewers to specify their
### search scope to a reasonable extent. Put differently, our Boolean search should contain at least 25
### search terms. On the other hand, the maximum search string length that Ovid, Proquest, Scopus, and Web of
### Science allow is around 1000 according to Gusenbauer & Haddaway (2020). Web of Science for example
### states that the maximum number of terms allowed in one field is 50 terms when using "All Fields". As
### such, this number of keywords is a natural cap to the number of keywords that we can seek to obtain from
### a corpus set. To reiterate, the number of keywords that will be incorporated in the final searches will
### have a lower limit of 25 search terms, and an upper limit of 1000 characters in the search string length,
### which translates to around 50 to 60 search terms (depending on the length of the search terms).
tagged_keywords <- litsearchr::extract_terms(
keywords = naive_dedup$keywords, # This is a list with the keywords from the articles.
method = "tagged", # Indicate that we want to obtain the keywords from the provided list.
min_freq = 20, # The keyword has to occur a minimum of twenty times to be included.
ngrams = TRUE, # 'litsearchr' should only consider an ngram with a minimum of length n,
min_n = 2, # where n is equal to 2.
language = "English") # 'litsearchr' should only consider English keywords.
length(tagged_keywords) # 101 tagged keywords.
### Now use the RAKE to obtain keywords from the titles and abstracts of naive search corpus. We extract all
### non-single keywords that occur at least twenty times in these titles and abstracts.
raked_keywords <- litsearchr::extract_terms(
text = paste(naive_dedup$title, naive_dedup$abstract), # This is a list of titles and abstracts
# per gold standard article.
method = "fakerake", # Indicate that 'litsearchr' should use the RAKE algorithm.
min_freq = 20, # The keyword has to occur a minimum of twenty times to be included.
ngrams = TRUE, # 'litsearchr' should only consider an ngram with a minimum of length n,
min_n = 2, # where n is equal to 2.
language = "English") # 'litsearchr' should only consider English keywords.
length(raked_keywords) # 235 raked keywords.
## Sum total of tagged and raked keywords.
keyword_candidates <- remove_redundancies(c(raked_keywords, tagged_keywords), closure = "full") # Remove
# duplicates.
length(keyword_candidates) # Total of 286 keyword candidates.
## The subsequent task is to select all keywords that are relevant to our query from the candidate pool. I
## select terms that relate in content to the relevant literature, i.e., are a independent or dependent
## variable of interest in the group contact on inter-ethnic attitudes relationship. Note that I interpret
## relevance quite broadly, since these terms will be will be ranked and filtered further based on their
## "connectedness" in a co-occurrence network. In that sense, it is better too be too liberal than too
## selective in our choices here, since it might be hard to anticipate which relevant terms are important
## from this "connectedness" perspective. I furthermore do not include keywords which relate directly to any
## of the other paradigms, e.g., I do not include keywords on group threat theory even though these might
## appear often in the group contact literature. Finally note that I do this part manually, which is prone to
## errors.
all_keywords <- keyword_candidates[
c(6, 18, 19, 20, 21, 22, 24, 26, 28, 32, 33, 35, 38, 41, 44, 45, 46, 47, 48, 49, 50, 55, 70, 72, 73, 74,
76, 77, 79, 80, 82, 83, 89, 90, 91, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 110, 111,
112, 115, 116, 117, 118, 120, 121, 123, 125, 127, 131, 132, 133, 134, 135, 136, 137, 140, 142, 144, 145,
149, 150, 151, 153, 157, 158, 168, 169, 170, 171, 172, 178, 180, 181, 182, 190, 193, 199, 200, 202, 204,
231, 237, 238, 239, 243, 244, 246, 247, 248, 255, 256, 257, 263, 267, 271, 272, 274, 276, 278, 279, 283,
285)
]
## Manual cleaning.
(all_keywords <- sort(all_keywords))
all_keywords[1] <- "group interactions" # Change "0410 group interactions" to "group interactions".
(all_keywords <- sort(all_keywords))
length(all_keywords) # 114 keyword candidates.
### We further filter the keyword set by ranking the relative strength of each keyword in a so-called keyword
### co-occurrence network (KCN). In a KCN, each keyword is "represented as a node and each co-occurrence of
### a pair of words is represented as a link" (Radhakrishnan, Erbis, Isaacs, & Kamarthi, 2017). "The number
### of times that a pair of words co-occurs in multiple articles constitutes the weight of the link
### connecting the pair" (Radhakrishnan, Erbis, Isaacs, & Kamarthi, 2017). "The network constructed in this
### manner represents cumulative knowledge of a domain and helps to uncover meaningful knowledge components
### and insights based on the patterns and strength of links between keywords that appear in the literature"
### (Radhakrishnan, Erbis, Isaacs, & Kamarthi, 2017). I furthermore combine the second iteration keywords
## with the naive keywords. I add the naive keywords to the keyword candidate selection under the assumption
## that these are important keywords because they were obtained from the gold standard articles, and should
## therefore be considered explicitly in the KCN analysis, even though they are implicitly represented.
all_keywords_final <- c()
all_keywords_final <- append(all_keywords_final, c(as.vector(unlist(gs_grouped_terms)), all_keywords))
all_keywords_final <- remove_redundancies(all_keywords_final, closure = "full") # Remove duplicates.
length(all_keywords_final) # 126 candidates.
## Build the keyword co-occurrence network. This chunk of code is a reworked version of the tutorial at:
## https://luketudge.github.io/litsearchr-tutorial/litsearchr_tutorial.html.
filter_dfm <- litsearchr::create_dfm(
elements = paste(naive_dedup$title, naive_dedup$abstract), # The input in which the keywords can co-occur,
# titles and abstracts.
features = all_keywords) # The keyword candidates.
kcn <- create_network(filter_dfm) # Create a KCN.
## Rank keywords based on strength in the co-occurrence network.
strengths <- strength(kcn) # Calculate strength values.
data.frame(term = names(strengths), strength = strengths, row.names = NULL) %>%
mutate(rank = rank(strength, ties.method = "min")) %>%
arrange(desc(strength)) ->
term_strengths # Create a data frame where the keywords are sorted by strength, in descending order.
## I now apply a filter to select 60 and 42 terms for the search string in ovid, scopus, and web of
## science, and ProQuest, respectively, where I remove terms which refer to an equivalent or near equivalent
## concept, i.e., "target group" and "target groups", where I select the term with the highest strength
## value.
(term_strengths <- cbind(seq(length(term_strengths$term)), term_strengths[order(term_strengths$term), ]))
term_strengths <- term_strengths[-c(2, 7, 9, 13, 20, 22, 32, 36, 37, 47, 52, 55, 62, 63, 66, 67, 73, 78, 89,
92, 99, 100, 101), ]
### Construct Boolean search OVID, Web of Science, and Scopus.
## Select first 60 terms from filtered term set.
keywords_ovid_scopus_wos <- as.character(term_strengths[order(term_strengths$strength, decreasing = T),
][1:60, ]$term)
(keywords_ovid_scopus_wos <- keywords_ovid_scopus_wos[order(keywords_ovid_scopus_wos)])
## Save the grouped terms to a text file in the working directory. I do this to keep the search reproducible
## in case some previous chunk of code does not replicate.
setwd("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iteration 1")
sink("first_iteration_selected_terms_ovid_scopus_wos.txt")
print(keywords_ovid_scopus_wos)
sink()
## Categorize "keywords_ovid_scopus_wos" object based on keyword being a potential dependent or independent
## variable of interest in the group threat on inter-ethnic attitudes relationship.
grouped_terms_ovid_scopus_wos <- list(
determinant = keywords_ovid_scopus_wos[c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20,
21, 24, 25, 26, 27, 29, 30, 31, 34, 35, 36, 37, 39, 40, 41, 43,
44, 45, 47, 49, 50, 54, 55, 56, 57, 58, 59, 60)],
outcome = keywords_ovid_scopus_wos[c(11, 22, 23, 28, 32, 33, 38, 42, 46, 48, 51, 52, 53)])
grouped_terms_ovid_scopus_wos
### Given the grouped terms, write the Boolean search for OVID, Web of Science, and Scopus to the
### directory and print it to the console.
setwd("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iteration 1") # Set directory.
write_search(
grouped_terms_ovid_scopus_wos, # The list of determinant and outcome keywords identified in the naive
# document set.
languages = "English", # Language to write the search in, here set to English.
exactphrase = TRUE, # Whether terms that consist of more than one word should be matched exactly rather
# than as two separate words. Set to TRUE, to limit both the scope and the number of redundant documents.
stemming = TRUE, # Whether words are stripped down to the smallest meaningful part of the word to catch all
# variants of the word. Set to TRUE.
closure = "none", # Whether partial matches are matched at the left end of a word ("left"), at the right
# ("right"), only as exact matches ("full"), or as any word containing a term ("none").  Set to "none" to
# obtain as many documents as possible.
writesearch = TRUE) # Whether we would like to write the search text to a file in the current directory.
# Set to TRUE.
bool <- capture.output(cat(read_file("search-inEnglish.txt")))
bool <- gsub('\\', "", bool, fixed = TRUE)
## Write the Boolean search for Ovid.
bool <- gsub("((", "(", bool, fixed = T)
bool <- gsub("))", ")", bool, fixed = T)
cat(bool)
writeLines(bool, "bool_ovid_it1.txt")
## Write the Boolean search for Scopus.
writeLines(bool, "bool_scopus_it1.txt")
## Write the Boolean search for Web of Science.
bool <- capture.output(cat(read_file("search-inEnglish.txt")))
bool <- paste0('ALL = ', bool, 'AND PY = (2010-2022)')
writeLines(bool, "bool_wos_it1.txt")
file.remove("search-inEnglish.txt") # Remove source file.
## Finally save the grouped terms to a text file in the working directory. I do this to keep the search
## reproducible in case some previous chunk of code does not replicate for any particular reason.
sink("first_iteration_search_terms_ovid_scopus_wos.txt")
print(grouped_terms_ovid_scopus_wos)
sink()
### Construct Boolean search ProQuest.
## Select first 42 terms from filtered term set.
keywords_proquest <- as.character(term_strengths[order(term_strengths$strength, decreasing = T),
][1:42, ]$term)
(keywords_proquest <- keywords_proquest[order(keywords_proquest)])
## Save the grouped terms to a text file in the working directory. I do this to keep the search reproducible
## in case some previous chunk of code does not replicate.
setwd("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iteration 1")
sink("first_iteration_selected_terms_proquest.txt")
print(keywords_proquest)
sink()
## Categorize "keywords_proquest" object based on keyword being a potential dependent or independent
## variable of interest in the group threat on inter-ethnic attitudes relationship.
grouped_terms_proquest <- list(
determinant = keywords_proquest[c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 19, 20, 21, 23,
24, 25, 26, 28, 29, 31, 32, 34, 36, 39, 40, 41, 42)],
outcome = keywords_proquest[c(16, 18, 22, 27, 30, 33, 35, 37, 38)])
grouped_terms_proquest
### Given the grouped terms, write the the Boolean search for ProQuest to the directory and print it to the
### console.
setwd("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iteration 1") # Set directory.
write_search(
grouped_terms_proquest, # The list of determinant and outcome keywords identified in the naive document
# set.
languages = "English", # Language to write the search in, here set to English.
exactphrase = TRUE, # Whether terms that consist of more than one word should be matched exactly rather
# than as two separate words. Set to TRUE, to limit both the scope and the number of redundant documents.
stemming = TRUE, # Whether words are stripped down to the smallest meaningful part of the word to catch all
# variants of the word. Set to TRUE.
closure = "none", # Whether partial matches are matched at the left end of a word ("left"), at the right
# ("right"), only as exact matches ("full"), or as any word containing a term ("none").  Set to "none" to
# obtain as many documents as possible.
writesearch = TRUE) # Whether we would like to write the search text to a file in the current directory.
bool <- capture.output(cat(read_file("search-inEnglish.txt")))
bool <- gsub('\\', "", bool, fixed = TRUE)
## Write the Boolean search for ProQuest.
bool <- gsub("((", "(", bool, fixed = T)
bool <- gsub("))", ")", bool, fixed = T)
cat(bool)
writeLines(bool, "bool_proquest_it1.txt")
file.remove("search-inEnglish.txt") # Remove source file.
## Finally save the grouped terms to a text file in the working directory. I do this to keep the search
## reproducible in case some previous chunk of code does not replicate for any particular reason.
sink("first_iteration_search_terms_proquest.txt")
print(grouped_terms_proquest)
sink()
### Data import and cleaning.
## Import results of informed search. Note that the batches for each search system were merged in EndNote.
## Start by checking whether the imported length is equal to the length of the raw .ris files.
import_ovid_it1 <- import_results(
directory = paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iterat",
"ion 1/1. Unmerged/Ovid"),
verbose = TRUE)
import_proquest_it1 <- import_results(
directory = paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iterat",
"ion 1/1. Unmerged/ProQuest"),
verbose = TRUE)
import_scopus_it1 <- import_results(
directory = paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iterat",
"ion 1/1. Unmerged/Scopus"),
verbose = TRUE)
import_wos_it1 <- import_results(
directory = paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iterat",
"ion 1/1. Unmerged/Web of Science"),
verbose = TRUE)
length(import_ovid_it1$title) # 2,466, which is correct.
length(import_proquest_it1$title) # 4,096, which is correct.
length(import_scopus_it1$title) # 4,347, which is correct.
length(import_wos_it1$title) # 4,749, which is correct.
### We subsequently identify and remove identifiable, non-English documents.
## Ovid.
table(import_ovid_it1$language) # Ovid. All documents in the .ris file are in English.
setwd(paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iteration 1/2. Me",
"rged/Ovid/1. Raw"))
write_refs(import_ovid_it1, format = "ris", tag_naming = "synthesisr", file = "ovid_r")
## ProQuest.
table(import_proquest_it1$language) # ProQuest. Not all documents in the .ris file are in English.
setwd(paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iteration 1/2. Me",
"rged/ProQuest/1. Raw"))
write_refs(import_proquest_it1, format = "ris", tag_naming = "synthesisr", file = "proquest_r")
## Scopus.
table(import_scopus_it1$language) # Scopus. All documents in the .ris file are in English.
setwd(paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iteration 1/2. Me",
"rged/Scopus/1. Raw"))
write_refs(import_scopus_it1, format = "ris", tag_naming = "synthesisr", file = "scopus_r")
## Web of Science.
table(import_wos_it1$language) # Web of Science. All documents in the .ris file are in English.
setwd(paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iteration 1/2. Me",
"rged/Web of Science/1. Raw"))
write_refs(import_wos_it1, format = "ris", tag_naming = "synthesisr", file = "web_of_science_r")
### Ovid.
## Exact matching.
length(import_ovid_it1$title) # Input is 2,466 documents.
exact_duplicates_ovid <- synthesisr::find_duplicates(
import_ovid_it1$title, # Raw import as input;
method = "exact", # method is optimal string alignment (Damerau-Levenshtein distance);
to_lower = TRUE, # convert input to lower case;
rm_punctuation = TRUE) # remove punctuation from input
(exact_manual <- synthesisr::review_duplicates(import_ovid_it1$title, exact_duplicates_ovid)) # Perform a
# manual check. One duplicate combination identified.
sum(as.numeric(table(exact_duplicates_ovid) - 1)) # One document should be removed.
it1_dedup_ovid <- synthesisr::extract_unique_references(import_ovid_it1, exact_duplicates_ovid)
length(it1_dedup_ovid$title) # Output after exact matching is 2,465. One document removed.
## Fuzzy matching five.
fuzzy_duplicates_ovid <- find_duplicates(
it1_dedup_ovid$title, # Exact matching output as input;
method = "string_osa", # method is optimal string alignment (Damerau-Levenshtein distance);
to_lower = TRUE, # convert input to lower case;
rm_punctuation = TRUE, # remove punctuation from input;
threshold = 5) # default threshold of five.
(fuzzy_manual <- synthesisr::review_duplicates(it1_dedup_ovid$title, fuzzy_duplicates_ovid)) # Perform a
# manual check. One duplicate combination identified.
sum(table(fuzzy_duplicates_ovid) - 1) # One document should be removed.
it1_dedup_ovid <- extract_unique_references(it1_dedup_ovid, fuzzy_duplicates_ovid) # Extract unique
# references.
length(it1_dedup_ovid$title) # De-duplicated output is 2,464. One document removed.
## Fuzzy matching ten.
fuzzy_duplicates_ovid <- find_duplicates(
it1_dedup_ovid$title, # Exact matching output as input;
method = "string_osa", # method is optimal string alignment (Damerau-Levenshtein distance);
to_lower = TRUE, # convert input to lower case;
rm_punctuation = TRUE, # remove punctuation from input;
threshold = 10) # increased threshold of ten.
(fuzzy_manual <- synthesisr::review_duplicates(it1_dedup_ovid$title, fuzzy_duplicates_ovid)) # Perform a
# manual check. No duplicate combinations identified.
length(it1_dedup_ovid$title) # De-duplicated output is 2,464. Zero documents removed.
## Save output as .ris file.
setwd(paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/1. Group contact/3. Iteration 1/2. Mer",
"ged/Ovid/2. Deduplicated"))
write_refs(it1_dedup_ovid, format = "ris", tag_naming = "synthesisr", file = "it1_dedup_ovid")
## Save output as .ris file.
setwd(paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iteration 1/2. Mer",
"ged/Ovid/2. Deduplicated"))
write_refs(it1_dedup_ovid, format = "ris", tag_naming = "synthesisr", file = "it1_dedup_ovid")
### ProQuest.
## Exact matching.
length(import_proquest_it1$title) # Input is 4,095 documents.
exact_duplicates_proquest <- synthesisr::find_duplicates(
import_proquest_it1$title, # Raw import as input;
method = "exact", # method is optimal string alignment (Damerau-Levenshtein distance);
to_lower = TRUE, # convert input to lower case;
rm_punctuation = TRUE) # remove punctuation from input
(exact_manual <- synthesisr::review_duplicates(import_proquest_it1$title, exact_duplicates_proquest)) # All
# combinations are duplicates.
sum(as.numeric(table(exact_duplicates_proquest) - 1)) # 110 articles should be removed.
it1_dedup_proquest <- extract_unique_references(import_proquest_it1, exact_duplicates_proquest)
length(it1_dedup_proquest$title) # Output after exact matching is 3,985. 110 documents removed.
## Fuzzy matching five.
fuzzy_duplicates_proquest <- find_duplicates(
it1_dedup_proquest$title, # Exact matching output as input;
method = "string_osa", # method is optimal string alignment (Damerau-Levenshtein distance);
to_lower = TRUE, # convert input to lower case;
rm_punctuation = TRUE, # remove punctuation from input;
threshold = 5) # default threshold of five.
(fuzzy_manual <- synthesisr::review_duplicates(it1_dedup_proquest$title, fuzzy_duplicates_proquest)) #
(fuzzy_manual <- synthesisr::review_duplicates(it1_dedup_proquest$title, fuzzy_duplicates_proquest)) #
# Perform a manual check. All combinations are duplicates.
sum(table(fuzzy_duplicates_proquest) - 1) # 18 documents should be removed.
it1_dedup_proquest <- extract_unique_references(it1_dedup_proquest, fuzzy_duplicates_proquest) #
# Extract unique references.
length(it1_dedup_proquest$title) # De-duplicated output is 3,967. 18 documents removed.
## Fuzzy matching ten.
fuzzy_duplicates_proquest <- find_duplicates(
it1_dedup_proquest$title, # Fuzzy matching five output as input;
method = "string_osa", # method is optimal string alignment (Damerau-Levenshtein distance);
to_lower = TRUE, # convert input to lower case;
rm_punctuation = TRUE, # remove punctuation from input;
threshold = 10) # increased threshold of ten.
(fuzzy_manual <- synthesisr::review_duplicates(it1_dedup_proquest$title, fuzzy_duplicates_proquest)) #
# Perform a manual check. One duplicate combination identified.
sum(table(fuzzy_duplicates_proquest) - 1) # Two documents should be removed.
it1_dedup_proquest <- extract_unique_references(it1_dedup_proquest, fuzzy_duplicates_proquest) # Extract
# unique references.
length(it1_dedup_proquest$title) # De-duplicated output is 3,965. Three documents removed.
## Save output as .ris file.
setwd(paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iteration 1/2. Mer",
"ged/ProQuest/2. Deduplicated"))
write_refs(it1_dedup_proquest, format = "ris", tag_naming = "synthesisr", file = "it1_dedup_proquest")
### Scopus.
## Exact matching.
length(import_scopus_it1$title) # Input is 4,347 documents.
exact_duplicates_scopus <- synthesisr::find_duplicates(
import_scopus_it1$title, # Raw import as input;
method = "exact", # method is optimal string alignment (Damerau-Levenshtein distance);
to_lower = TRUE, # convert input to lower case;
rm_punctuation = TRUE) # remove punctuation from input
(exact_manual <- synthesisr::review_duplicates(import_scopus_it1$title, exact_duplicates_scopus)) # Perform a
# manual check. Four duplicate combinations identified.
sum(as.numeric(table(exact_duplicates_scopus) - 1)) # Four documents should be removed.
it1_dedup_scopus <- synthesisr::extract_unique_references(import_scopus_it1, exact_duplicates_scopus)
length(it1_dedup_scopus$title) # Output after exact matching is 4,343 . Seven documents removed.
## Fuzzy matching five.
fuzzy_duplicates_scopus <- find_duplicates(
it1_dedup_scopus$title, # Exact matching output as input;
method = "string_osa", # method is optimal string alignment (Damerau-Levenshtein distance);
to_lower = TRUE, # convert input to lower case;
rm_punctuation = TRUE, # remove punctuation from input;
threshold = 5) # default threshold of five.
(fuzzy_manual <- synthesisr::review_duplicates(it1_dedup_scopus$title, fuzzy_duplicates_scopus)) # Perform a
# manual check. Zero duplicate combinations identified.
sum(table(fuzzy_duplicates_scopus) - 1) # Two documents should be removed.
it1_dedup_scopus <- extract_unique_references(it1_dedup_scopus, fuzzy_duplicates_scopus) # Extract unique
# references.
length(it1_dedup_scopus$title) # De-duplicated output is 4,341. One document removed.
## Fuzzy matching ten.
fuzzy_duplicates_scopus <- find_duplicates(
it1_dedup_scopus$title, # Exact matching output as input;
method = "string_osa", # method is optimal string alignment (Damerau-Levenshtein distance);
to_lower = TRUE, # convert input to lower case;
rm_punctuation = TRUE, # remove punctuation from input;
threshold = 10) # increased threshold of ten.
(fuzzy_manual <- synthesisr::review_duplicates(it1_dedup_scopus$title, fuzzy_duplicates_scopus)) # Perform a
# manual check. No duplicate combinations identified.
length(it1_dedup_scopus$title) # De-duplicated output is 4,341. Zero documents removed.
## Save output as .ris file.
setwd(paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iteration 1/2. Mer",
"ged/Scopus/2. Deduplicated"))
write_refs(it1_dedup_scopus, format = "ris", tag_naming = "synthesisr", file = "it1_dedup_scopus")
### Web of Science.
length(import_wos_it1$title) # Input is 4,749 documents.
exact_duplicates_wos <- synthesisr::find_duplicates(
import_wos_it1$title, # Raw import as input;
method = "exact", # method is optimal string alignment (Damerau-Levenshtein distance);
to_lower = TRUE, # convert input to lower case;
rm_punctuation = TRUE) # remove punctuation from input
(exact_manual <- synthesisr::review_duplicates(import_wos_it1$title, exact_duplicates_wos)) # Perform a
# manual check. Three duplicate combinations identified.
sum(as.numeric(table(exact_duplicates_wos) - 1)) # Five documents should be removed.
it1_dedup_wos <- synthesisr::extract_unique_references(import_wos_it1, exact_duplicates_wos)
length(it1_dedup_wos$title) # Output after exact matching is 4,744. Four documents removed.
## Fuzzy matching five.
fuzzy_duplicates_wos <- find_duplicates(
it1_dedup_wos$title, # Exact matching output as input;
method = "string_osa", # method is optimal string alignment (Damerau-Levenshtein distance);
to_lower = TRUE, # convert input to lower case;
rm_punctuation = TRUE, # remove punctuation from input;
threshold = 5) # default threshold of five.
(fuzzy_manual <- synthesisr::review_duplicates(it1_dedup_wos$title, fuzzy_duplicates_wos)) # Perform a
# manual check. No duplicate combinations identified.
sum(table(fuzzy_duplicates_wos) - 1) # Zero documents should be removed.
it1_dedup_wos <- extract_unique_references(it1_dedup_wos, fuzzy_duplicates_wos) # Extract unique
# references.
length(it1_dedup_wos$title) # De-duplicated output is 4,744. One document removed.
## Save output as .ris file.
setwd(paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iteration 1/2. Mer",
"ged/Web of Science/2. Deduplicated"))
write_refs(it1_dedup_wos, format = "ris", tag_naming = "synthesisr", file = "it1_dedup_wos")
### Investigate the distribution of the overlap in the first iteration. Please note that this is very
### approximate, in that I am not sure how this function matches duplicates (I assume its exact).
ggVennDiagram(list(it1_dedup_ovid$title, it1_dedup_proquest$title, it1_dedup_scopus$title,
it1_dedup_wos$title),
category.names = c("Ovid", "ProQuest", "Scopus", "Web of Science"), label_alpha = 0.75,
label = c("count"))
## Gold standard precision check. For convenience I assume that similarity scores > 0.5 are a match, and
## those =< 0.5 are not.
cbind(gs_group_contact$title, gs_group_contact$year)
## Ovid.
sum(round(as.numeric(check_recall(gs_group_contact$title[c(2, 4)], it1_dedup_ovid$title)[, 3]), 0)) # 0 of 2
# gold standard articles are retrieved.
## ProQuest.
sum(round(as.numeric(check_recall(gs_group_contact$title[c(2, 4)], it1_dedup_proquest$title)[, 3]), 0)) # 2
# of 2 gold standard articles are retrieved.
## Scopus.
sum(round(as.numeric(check_recall(gs_group_contact$title[c(2, 4)], it1_dedup_scopus$title)[, 3]), 0)) # 1 of
# 2 gold standard articles are retrieved.
## Web of Science.
sum(round(as.numeric(check_recall(gs_group_contact$title[c(2, 4)], it1_dedup_wos$title)[, 3]), 0)) # 2 of 2
## Ovid.
sum(round(as.numeric(check_recall(gs_group_contact$title, it1_dedup_ovid$title)[, 3]), 0)) # 0 of 2
# gold standard articles are retrieved.
## ProQuest.
sum(round(as.numeric(check_recall(gs_group_contact$title, it1_dedup_proquest$title)[, 3]), 0)) # 2
# of 2 gold standard articles are retrieved.
## Scopus.
sum(round(as.numeric(check_recall(gs_group_contact$title, it1_dedup_scopus$title)[, 3]), 0)) # 1 of
# 2 gold standard articles are retrieved.
## Web of Science.
sum(round(as.numeric(check_recall(gs_group_contact$title, it1_dedup_wos$title)[, 3]), 0)) # 2 of 2
# standard articles are retrieved.
## ProQuest.
sum(round(as.numeric(check_recall(gs_group_contact$title, it1_dedup_proquest$title)[, 3]), 0)) # 2
### Remove duplicates between corpora and combine the various corpora into a .ris file.
it1_dedup <- bind_rows(it1_dedup_ovid, it1_dedup_proquest, it1_dedup_scopus, it1_dedup_wos) # Merge corpora.
## Exact matching.
length(it1_dedup$title) # Input is 15,514 documents.
exact_duplicates <- synthesisr::find_duplicates(
it1_dedup$title, # Raw import as input;
method = "exact", # method is optimal string alignment (Damerau-Levenshtein distance);
to_lower = TRUE, # convert input to lower case;
rm_punctuation = TRUE) # remove punctuation from input
exact_manual <- synthesisr::review_duplicates(it1_dedup$title, exact_duplicates) # Perform a
# manual check.
length(exact_manual$title) # Sum of 8,707 duplicate combinations identified.
sum(as.numeric(table(exact_duplicates) - 1)) # 5,317 documents should be removed.
it1_dedup <- synthesisr::extract_unique_references(it1_dedup, exact_duplicates)
length(it1_dedup$title) # Output after exact matching is 10,197. 5,317 documents removed.
## Fuzzy matching five.
fuzzy_duplicates <- find_duplicates(
it1_dedup$title, # Exact match documents as input;
method = "string_osa", # method is optimal string alignment (Damerau-Levenshtein distance);
to_lower = TRUE, # convert input to lower case;
rm_punctuation = TRUE, # remove punctuation from input;
threshold = 5) # default cutoff.
fuzzy_manual <- review_duplicates(it1_dedup$title, fuzzy_duplicates) # Perform a manual check.
length(fuzzy_manual$title) # 1,004 potential duplicate combinations. I check these candidates manually in
View(gs_group_contact)
