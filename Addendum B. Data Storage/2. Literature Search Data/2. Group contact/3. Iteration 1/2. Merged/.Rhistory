set.seed(3791) # One ring to rule them all. In principle, there is no RNG in this script, but I set a seed
# anyway.
### Start by generating a set of naive keywords. This set will be used to perform an initial, naive search of
### the group contact literature. More specifically, we limit the scope of the search to the direct and
### extended group contact literature. The naive keywords are extracted from two sources, the review
### question, and a set of gold-standard articles. The review question is formulated as: are group threat,
## group contact, media, socialization, and the demographics age, gender, and education determinants of
### inter-ethnic attitudes in the 2010-2022 period? Note that we identify four research paradigms in this
### research question: group threat, group contact, media, and socialization. Effects of the various
### demographic variables are assumed to be present in articles on the four main paradigms. In this specific
### R-file we limit the scope to the group contact determinant. We split the constituent elements in the
### research question as being either a determinant or an outcome of interest:
naive_keywords <- c("group contact",  "inter-ethnic attitudes")
### The second naive keyword source are a set of six gold standard articles.
## Importing the set of gold standard articles from the corresponding directory.
gs_group_contact <- import_results(
directory = paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/1. Valida",
"tion/1. Gold standard"),
verbose = TRUE)
## Request the length of the title object to show that there are 6 gold standard articles. Also request the
## object itself to show the titles of the articles themselves.
length(gs_group_contact$title)
gs_group_contact$title
### The keywords embedded in these gold standard articles are the keywords as they have been listed by the
### authors themselves, and those keywords as identified by the Rapid Automatic Keyword Extraction Algorithm
### (RAKE). The RAKE is a keyword extraction algorithm which tries to determine key phrases in a body of text
### by analyzing the frequency of word appearance and its co-occurrence with other words in the text. Please
### refer to Rose, Engel, Cramer, and Cowley (2010) for an overview of the RAKE.
## Start by obtaining the keywords listed in the articles. We extract all non-single keywords that are listed
## at least once. Single word keywords were inspected for viability but excluded.
sum(!is.na(gs_group_contact$title)) # Note that all of the 6 gold standard articles list keywords.
gs_tagged_keywords <- litsearchr::extract_terms(
keywords = gs_group_contact$keywords, # This is a list with the keywords from the gold standard articles.
method = "tagged", # Indicate that we want to obtain the keywords from the provided list.
min_freq = 1, # The keyword has to occur a minimum of one time to be included.
ngrams = TRUE, # 'litsearchr' should only consider an ngram with a minimum of length n,
min_n = 1, # where n is equal to 2.
language = "English") # 'litsearchr' should only consider English keywords.
gs_tagged_keywords # Resulting keywords.
## I subsequently make a selection based on the degree to which each of the keywords is relevant to the
## effect of contact on inter ethnic attitudes. Note that I interpret this broadly. I exclude "behavior",
# "children", "ethnic  diversity", "europe", "homosexuals", "identity", "immigration", "in-group",
# "judgments", "mediating role", "minority", "negative  contact", "netherlands", "out-group", "personality",
# "predictors", "preferences", "reconciliation", "reduction", "school", "sentiments", "threat", "trust", and
# "violence". I remove  "ethnic  diversity" and "negative  contact" because these same terms are listed
# without redundant spacing. I exclude terms like "identity" and "immigration" because although these are
# arguably somewhat relevant terms, they are much too broad for a contact theoretical specific search.
gs_tagged_keywords <- gs_tagged_keywords[c(1, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 24, 25, 26,
28, 32, 33, 36, 37, 39, 42, 43, 44, 47, 48, 50, 52, 53, 54,
56)]
## Remove redundant spacing and characters.
gs_tagged_keywords[7] <- "cultural embeddedness"
gs_tagged_keywords[8] <- "ethnic group"
gs_tagged_keywords[20] <- "outgroup size"
gs_tagged_keywords[21] <- "perceived diversity"
gs_tagged_keywords[24] <- "public attitudes"
gs_tagged_keywords[25] <- "racial attitudes"
gs_tagged_keywords[30] <- "social cohesion"
gs_tagged_keywords
## Use the RAKE to obtain keywords from the titles and abstracts of the gold standard articles. We extract
## keywords that occur at least once in the titles and abstracts.
gs_raked_keywords <- litsearchr::extract_terms(
text = paste(gs_group_contact$title, gs_group_contact$abstract), # This is a list of the gold standard
# articles' titles and abstracts.
method = "fakerake", # Indicate that 'litsearchr' should use the RAKE algorithm.
min_freq = 1, # The keyword has to occur a minimum of two times to be included.
ngrams = TRUE, # 'litsearchr' should only consider an ngram with a minimum of length n,
min_n = 1, # where n is equal to 2.
language = "English") # 'litsearchr' should only consider English keywords.
gs_raked_keywords
## I subsequently make a selection based on the degree to which each of the keywords is relevant to the
## direct and extended contact on inter-ethnic attitudes relationship. Note that I interpret this broadly.
gs_raked_keywords <- gs_raked_keywords[c(13, 24, 40, 49, 66, 70, 81, 85, 93, 95, 112, 124, 126, 145, 147,
148, 150, 182, 184, 217, 238, 248, 249, 251, 277, 287, 289, 290,
292, 316, 317, 328, 351, 355, 358, 384, 385, 404, 407, 409, 413,
415, 417, 423, 424, 453, 494, 498, 568, 570)]
gs_raked_keywords # The resulting keywords.
## Remove redundant spacing and characters.
gs_raked_keywords[6] <- "contact valence"
gs_raked_keywords[45] <- "prejudice reduction"
## Combine the tagged and raked keywords from the gold standard articles.
gs_all_keywords <- c()
gs_all_keywords <- append(gs_all_keywords, c(gs_tagged_keywords, gs_raked_keywords))
gs_all_keywords <- sort(gs_all_keywords)
gs_all_keywords <- remove_redundancies(gs_all_keywords, closure = "full") # Remove duplicate search terms.
## Filter "gs_all_keywords" object. I do this on the basis of the keyword being either a dependent or
## independent variable of interest, or being a prominent keywords in the title or abstract.
## Important title keywords.
gs_group_contact$title[1] # "diverse societies", "cohesion", "contact theory", "mediated contact theory", are
# prominent keywords.
gs_group_contact$title[2] # "intergroup contact", "prejudice toward immigrants", "individual conservative
# values", "cultural embeddedness", are prominent keywords.
gs_group_contact$title[3] # "countervailing contact", "ethnic diversity", "anti immigrant attitudes",
# "positive inter-ethnic contact, "negative inter-ethnic contact", are prominent keywords.
gs_group_contact$title[4] # "xenophobia", "inter-ethnic contact", are prominent keywords.
gs_group_contact$title[5] # "social contact", "prejudice", "discrimination", are prominent keywords.
gs_group_contact$title[6] # "extended contact", "direct contact", "group norms", "positive ethnic intergroup
# attitudes", are prominent keywords.
## Important abstract keywords.
gs_group_contact$abstract[1] # "ethnic diversity", "social cohesion", "positive intergroup contact", are
# prominent keywords.
gs_group_contact$abstract[2] # "individual conservative values", "cultural embeddedness", "contact with
# immigrants", "attitudes toward immigrants", "ethnic prejudice', "prejudice", are prominent keywords.
gs_group_contact$abstract[3] # "inter-ethnic contact", "contact-valence", "attitudes towards immigrants",
# "inter-group contact, "positive inter-group contact", "negative inter-group contact", "diverse
# communities", are prominent keywords.
gs_group_contact$abstract[4] # "xenophobic attitudes", "xenophobia", "inter-ethnic contact", "positive
# inter-ethnic contact", "negative inter-ethnic contact", are prominent keywords.
gs_group_contact$abstract[5] # "positive social contact", "social contact", "positive integroup social
# contact", "prejudice", "discrimination", are prominent keywords.
gs_group_contact$abstract[6] # "direct contact", "extended contact", "in-group norms", "out-group norms",
# "cross-ethnic friendships", are prominent keywords.
## Making a selection on whether a keyword is either a dependent or independent variable of interest, or is
## prominent in the title or abstract of the gold standard articles.
gs_grouped_terms <- list(
determinant = gs_all_keywords[c(8, 9, 11, 15, 25, 26, 33, 35, 37, 39, 40, 42, 43, 44, 50, 52, 53)],
outcome = c(gs_all_keywords[c(18, 21, 23, 28, 34, 51, 56, 57, 58, 59, 70)]))
gs_grouped_terms # Final set of naive keywords.
## Save .ris file of the merged file.
setwd("C:\\Academia\\PhD\\Meta-analysis paper\\Literature data\\Group threat\\3. Iteration 1")
# Zotero indicates that 4 duplicates remain in the .ris file after the de-duplication procedure. These
# documents were removed manually in Zotero, for a final sum total of 7,169 documents. After manually
# removing the duplicate documents from the "it1_dedup_out.ris" in Zotero, I overwrite the original
# "it1_dedup_out.ris" file with it, which I then import.
it1_dedup_out <- read_bibliography("it1_dedup_out.ris")
## Save .ris file of the merged file.
setwd("C:\\Academia\\PhD\\Meta-analysis paper\\Literature data\\Group threat\\3. Iteration 1")
## Save .ris file of the merged file.
setwd("C:\\Academia\\PhD\\Meta-analysis paper\\Literature data\\2. Group threat\\3. Iteration 1")
## Save .ris file of the merged file.
setwd("C:\\Academia\\PhD\\Meta-analysis paper\\Literature data\\2. Group contact\\3. Iteration 1")
# Zotero indicates that 4 duplicates remain in the .ris file after the de-duplication procedure. These
# documents were removed manually in Zotero, for a final sum total of 7,169 documents. After manually
# removing the duplicate documents from the "it1_dedup_out.ris" in Zotero, I overwrite the original
# "it1_dedup_out.ris" file with it, which I then import.
it1_dedup_out <- read_bibliography("it1_dedup_out.ris")
length(it1_dedup_out$title) # 7,169 documents.  documents removed.
#############################################
##### Literature Search - Group Contact #####
#############################################
####################
##### Packages #####
####################
## Package names
packages <- c("devtools", "dplyr", "igraph",  "readr", "remotes", "revtools", "synthesisr")
## Install packages that are not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
## Load packages.
invisible(lapply(packages, library, character.only = TRUE))
## Check if "litsearchr" and "ggVennDiagram" packages can be required. If not, install it.
if(!require(litsearchr)) install_github("elizagrames/litsearchr", ref = "main")
if (!require(ggVennDiagram)) install_github("gaospecial/ggVennDiagram")
## Require "litsearchr" and "ggVennDiagram" packages.
library(litsearchr)
library(ggVennDiagram)
## Clear environment.
rm(list = ls())
###################################
##### Naive Literature Search #####
###################################
set.seed(3791) # One ring to rule them all. In principle, there is no RNG in this script, but I set a seed
# anyway.
### Start by generating a set of naive keywords. This set will be used to perform an initial, naive search of
### the group contact literature. More specifically, we limit the scope of the search to the direct and
### extended group contact literature. The naive keywords are extracted from two sources, the review
### question, and a set of gold-standard articles. The review question is formulated as: are group threat,
## group contact, media, socialization, and the demographics age, gender, and education determinants of
### inter-ethnic attitudes in the 2010-2022 period? Note that we identify four research paradigms in this
### research question: group threat, group contact, media, and socialization. Effects of the various
### demographic variables are assumed to be present in articles on the four main paradigms. In this specific
### R-file we limit the scope to the group contact determinant. We split the constituent elements in the
### research question as being either a determinant or an outcome of interest:
naive_keywords <- c("group contact",  "inter-ethnic attitudes")
### The second naive keyword source are a set of six gold standard articles.
## Importing the set of gold standard articles from the corresponding directory.
gs_group_contact <- import_results(
directory = paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/1. Valida",
"tion/1. Gold standard"),
verbose = TRUE)
## Request the length of the title object to show that there are 6 gold standard articles. Also request the
## object itself to show the titles of the articles themselves.
length(gs_group_contact$title)
gs_group_contact$title
### The keywords embedded in these gold standard articles are the keywords as they have been listed by the
### authors themselves, and those keywords as identified by the Rapid Automatic Keyword Extraction Algorithm
### (RAKE). The RAKE is a keyword extraction algorithm which tries to determine key phrases in a body of text
### by analyzing the frequency of word appearance and its co-occurrence with other words in the text. Please
### refer to Rose, Engel, Cramer, and Cowley (2010) for an overview of the RAKE.
## Start by obtaining the keywords listed in the articles. We extract all non-single keywords that are listed
## at least once. Single word keywords were inspected for viability but excluded.
sum(!is.na(gs_group_contact$title)) # Note that all of the 6 gold standard articles list keywords.
gs_tagged_keywords <- litsearchr::extract_terms(
keywords = gs_group_contact$keywords, # This is a list with the keywords from the gold standard articles.
method = "tagged", # Indicate that we want to obtain the keywords from the provided list.
min_freq = 1, # The keyword has to occur a minimum of one time to be included.
ngrams = TRUE, # 'litsearchr' should only consider an ngram with a minimum of length n,
min_n = 1, # where n is equal to 2.
language = "English") # 'litsearchr' should only consider English keywords.
gs_tagged_keywords # Resulting keywords.
## I subsequently make a selection based on the degree to which each of the keywords is relevant to the
## effect of contact on inter ethnic attitudes. Note that I interpret this broadly. I exclude "behavior",
# "children", "ethnic  diversity", "europe", "homosexuals", "identity", "immigration", "in-group",
# "judgments", "mediating role", "minority", "negative  contact", "netherlands", "out-group", "personality",
# "predictors", "preferences", "reconciliation", "reduction", "school", "sentiments", "threat", "trust", and
# "violence". I remove  "ethnic  diversity" and "negative  contact" because these same terms are listed
# without redundant spacing. I exclude terms like "identity" and "immigration" because although these are
# arguably somewhat relevant terms, they are much too broad for a contact theoretical specific search.
gs_tagged_keywords <- gs_tagged_keywords[c(1, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 24, 25, 26,
28, 32, 33, 36, 37, 39, 42, 43, 44, 47, 48, 50, 52, 53, 54,
56)]
## Remove redundant spacing and characters.
gs_tagged_keywords[7] <- "cultural embeddedness"
gs_tagged_keywords[8] <- "ethnic group"
gs_tagged_keywords[20] <- "outgroup size"
gs_tagged_keywords[21] <- "perceived diversity"
gs_tagged_keywords[24] <- "public attitudes"
gs_tagged_keywords[25] <- "racial attitudes"
gs_tagged_keywords[30] <- "social cohesion"
gs_tagged_keywords
## Use the RAKE to obtain keywords from the titles and abstracts of the gold standard articles. We extract
## keywords that occur at least once in the titles and abstracts.
gs_raked_keywords <- litsearchr::extract_terms(
text = paste(gs_group_contact$title, gs_group_contact$abstract), # This is a list of the gold standard
# articles' titles and abstracts.
method = "fakerake", # Indicate that 'litsearchr' should use the RAKE algorithm.
min_freq = 1, # The keyword has to occur a minimum of two times to be included.
ngrams = TRUE, # 'litsearchr' should only consider an ngram with a minimum of length n,
min_n = 1, # where n is equal to 2.
language = "English") # 'litsearchr' should only consider English keywords.
gs_raked_keywords
## I subsequently make a selection based on the degree to which each of the keywords is relevant to the
## direct and extended contact on inter-ethnic attitudes relationship. Note that I interpret this broadly.
gs_raked_keywords <- gs_raked_keywords[c(13, 24, 40, 49, 66, 70, 81, 85, 93, 95, 112, 124, 126, 145, 147,
148, 150, 182, 184, 217, 238, 248, 249, 251, 277, 287, 289, 290,
292, 316, 317, 328, 351, 355, 358, 384, 385, 404, 407, 409, 413,
415, 417, 423, 424, 453, 494, 498, 568, 570)]
gs_raked_keywords # The resulting keywords.
## Remove redundant spacing and characters.
gs_raked_keywords[6] <- "contact valence"
gs_raked_keywords[45] <- "prejudice reduction"
## Combine the tagged and raked keywords from the gold standard articles.
gs_all_keywords <- c()
gs_all_keywords <- append(gs_all_keywords, c(gs_tagged_keywords, gs_raked_keywords))
gs_all_keywords <- sort(gs_all_keywords)
gs_all_keywords <- remove_redundancies(gs_all_keywords, closure = "full") # Remove duplicate search terms.
## Filter "gs_all_keywords" object. I do this on the basis of the keyword being either a dependent or
## independent variable of interest, or being a prominent keywords in the title or abstract.
## Important title keywords.
gs_group_contact$title[1] # "diverse societies", "cohesion", "contact theory", "mediated contact theory", are
# prominent keywords.
gs_group_contact$title[2] # "intergroup contact", "prejudice toward immigrants", "individual conservative
# values", "cultural embeddedness", are prominent keywords.
gs_group_contact$title[3] # "countervailing contact", "ethnic diversity", "anti immigrant attitudes",
# "positive inter-ethnic contact, "negative inter-ethnic contact", are prominent keywords.
gs_group_contact$title[4] # "xenophobia", "inter-ethnic contact", are prominent keywords.
gs_group_contact$title[5] # "social contact", "prejudice", "discrimination", are prominent keywords.
gs_group_contact$title[6] # "extended contact", "direct contact", "group norms", "positive ethnic intergroup
# attitudes", are prominent keywords.
## Important abstract keywords.
gs_group_contact$abstract[1] # "ethnic diversity", "social cohesion", "positive intergroup contact", are
# prominent keywords.
gs_group_contact$abstract[2] # "individual conservative values", "cultural embeddedness", "contact with
# immigrants", "attitudes toward immigrants", "ethnic prejudice', "prejudice", are prominent keywords.
gs_group_contact$abstract[3] # "inter-ethnic contact", "contact-valence", "attitudes towards immigrants",
# "inter-group contact, "positive inter-group contact", "negative inter-group contact", "diverse
# communities", are prominent keywords.
gs_group_contact$abstract[4] # "xenophobic attitudes", "xenophobia", "inter-ethnic contact", "positive
# inter-ethnic contact", "negative inter-ethnic contact", are prominent keywords.
gs_group_contact$abstract[5] # "positive social contact", "social contact", "positive integroup social
# contact", "prejudice", "discrimination", are prominent keywords.
gs_group_contact$abstract[6] # "direct contact", "extended contact", "in-group norms", "out-group norms",
# "cross-ethnic friendships", are prominent keywords.
## Making a selection on whether a keyword is either a dependent or independent variable of interest, or is
## prominent in the title or abstract of the gold standard articles.
gs_grouped_terms <- list(
determinant = gs_all_keywords[c(8, 9, 11, 15, 25, 26, 33, 35, 37, 39, 40, 42, 43, 44, 50, 52, 53)],
outcome = c(gs_all_keywords[c(18, 21, 23, 28, 34, 51, 56, 57, 58, 59, 70)]))
gs_grouped_terms # Final set of naive keywords.
## Save .ris file of the merged file.
setwd("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/2. Naive search/2. Merged")
# EndNote and Zotero both indicate that seven duplicates remain in the .ris file. We furthermore remove three
# retracted papers: "Interprofessional learning in acute care: Developing a theoretical framework",
# "Bridging the Gap on Facebook: Assessing Intergroup Contact and Its Effects for Intergroup Relations", and
# "When contact changes minds: An experiment on transmission of support for gay equality". The second paper
# has two documents in the set, one on the paper itself, and one on the retraction. The other two have just
# one paper. The result is exported in the file "naive_dedup.ris" which is subsequently imported.
naive_dedup <- read_bibliography("naive_dedup.ris")
length(naive_dedup$title) # 2,043 documents. 11 documents removed.
## Save .ris file.
setwd("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/3. Iteration 1")
# as .bib for Zotero.
# EndNote and Zotero indicate that two duplicates remain in the .ris file, which are removed. In addition,
# one more retraction was identified: "Ethnic threat and social control: Examining public support for
# judicial use of ethnicity in punishment", consisting of two documents, which was removed. The result is
# exported from Zotero in the file "it1_dedup_out.ris" which is subsequently imported.
it1_dedup_out <- read_bibliography("it1_dedup_out.ris")
length(it1_dedup_out$title) # 6,410 documents. Four documents removed. This is the final corpus file of the
### Clear the environment except for the gold standard articles, "naive_dedup", and "it1_dedup_out" objects.
rm(list = setdiff(ls(), c("gs_grouped_terms", "gs_group_contact", "naive_dedup", "it1_dedup_out")))
## External precision check. For convenience I assume that similarity scores > 0.5 are a match, and those
## =< 0.5 are not.
ex_group_contact <- import_results(
directory = paste0("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group contact/1. Validat",
"ion/2. External"),
verbose = TRUE)
### Start by checking how many documents in the first iteration search corpus provide keywords, titles, and
### abstracts.
## Keywords.
length(it1_dedup_out$keywords) # Total number of documents is 9,841.
## Inspect coverage gain relative to the naive search. I define coverage as the sum of the additional
## documents that were found relative to the previous search iteration, and the documents from the previous
## search iteration that were not identified in the current one.
length(naive_dedup$title) # 2,043 articles.
length(it1_dedup$title) # 5,578 articles.
## Save .ris file of the merged file.
setwd("C:/Academia/PhD/Meta-analysis paper/Literature data/2. Group Contact/3. Iteration 1/2. Merged")
# EndNote and Zotero indicate that  duplicates remain in the .ris file, which are removed. We furthermore
# remove four retracted papers that are flagged in EndNote and Zotero: "Interprofessional learning in acute care:
# Developing a theoretical framework", "The Evolution of Intergroup Bias: Perceptions and Attitudes in
# Rhesus Macaques", and "Bridging the Gap on Facebook: Assessing Intergroup Contact and Its Effects for
# Intergroup Relations". The second paper had three records, because two retraction statements were included,
# where the final paper had four, because it was a duplicate, each of which had a retraction statement. The
# result is exported from Zotero in the file "it1_dedup.ris" which is subsequently imported.
it1_dedup <- read_bibliography("it1_dedup.ris")
length(it1_dedup$title) # 9,476 documents. 50 documents removed.
## Inspect coverage gain relative to the naive search. I define coverage as the sum of the additional
## documents that were found relative to the previous search iteration, and the documents from the previous
## search iteration that were not identified in the current one.
length(naive_dedup$title) # 2,043 articles.
length(it1_dedup$title) # 5,578 articles.
(length(it1_dedup$title) - length(naive_dedup$title)) # 5,578 - 2,043 = 3,535 more documents identified.
(length(it1_dedup_out$title) - length(it1_dedup$title)) # ~832 documents from the naive search not in the
# first iteration search.
# So total coverage increase is: 3,535 + 832 = 4,367 documents or ((4,367 - 2,043) / 2,043) * 100 =
# 113.75% as a percentage of the naive document set which is a factor increase of 4,367 / 2,043 = 2.14.
(1 - (length(it1_dedup_out$title) - length(it1_dedup$title)) / length(naive_dedup$title)) * 100 # 59.28% of
### Clear the environment except for the gold standard search terms, the validation article sets, and
### the "naive_dedup", and "it1_dedup_out" objects.
rm(list = setdiff(ls(), c("gs_grouped_terms", "gs_group_contact", "ex_group_contact", "naive_dedup",
"it1_dedup_out")))
### Start by checking how many documents in the first iteration search corpus provide keywords, titles, and
### abstracts.
## Keywords.
length(it1_dedup_out$keywords) # Total number of documents is 9,841.
length(it1_dedup_out$keywords) - sum(is.na(it1_dedup_out$keywords)) # All of the articles list keywords.
## Titles and abstracts.
length(it1_dedup_out$title) - sum(is.na(it1_dedup_out$title)) # All of the articles list a title.
length(it1_dedup_out$abstract) - sum(is.na(it1_dedup_out$abstract)) # All of the articles list an abstract.
### Gusenbauer & Haddaway (2020) note that search terms of length twenty-five should allow reviewers to
### specify theirsearch scope to a reasonable extent. Put differently, our Boolean search should contain at
### least twenty-five search terms. On the other hand, the maximum search string length that Ovid, Scopus,
### and Web of Science allow according to Gusenbauer & Haddaway (2020) is around 1000. Web of Science for
### example states that the maximum number of terms allowed in one field is fifty terms when using
### "All Fields". As such, this number of keywords is a natural cap to the number of keywords that we can
### seek to obtain from a corpora set. To reiterate, the number of keywords that will be incorporated in the
### final searches will have a lower limit of 25 search terms, and an upper limit of 1000 characters in the
### search string length, which translates to around fifty to sixty search terms (depending on the length of
### the search terms).
tagged_keywords <- litsearchr::extract_terms(
keywords = it1_dedup_out$keywords, # This is a list with the keywords from the articles.
method = "tagged", # Indicate that we want to obtain the keywords from the provided list.
min_freq = 40, # The keyword has to occur a minimum of forty times to be included.
ngrams = TRUE, # 'litsearchr' should only consider an ngram with a minimum of length n,
min_n = 2, # where n is equal to 2.
language = "English") # 'litsearchr' should only consider English keywords.
length(tagged_keywords) # 300 tagged keywords.
### Gusenbauer & Haddaway (2020) note that search terms of length twenty-five should allow reviewers to
### specify theirsearch scope to a reasonable extent. Put differently, our Boolean search should contain at
### least twenty-five search terms. On the other hand, the maximum search string length that Ovid, Scopus,
### and Web of Science allow according to Gusenbauer & Haddaway (2020) is around 1000. Web of Science for
### example states that the maximum number of terms allowed in one field is fifty terms when using
### "All Fields". As such, this number of keywords is a natural cap to the number of keywords that we can
### seek to obtain from a corpora set. To reiterate, the number of keywords that will be incorporated in the
### final searches will have a lower limit of 25 search terms, and an upper limit of 1000 characters in the
### search string length, which translates to around fifty to sixty search terms (depending on the length of
### the search terms).
tagged_keywords <- litsearchr::extract_terms(
keywords = it1_dedup_out$keywords, # This is a list with the keywords from the articles.
method = "tagged", # Indicate that we want to obtain the keywords from the provided list.
min_freq = 50, # The keyword has to occur a minimum of forty times to be included.
ngrams = TRUE, # 'litsearchr' should only consider an ngram with a minimum of length n,
min_n = 2, # where n is equal to 2.
language = "English") # 'litsearchr' should only consider English keywords.
length(tagged_keywords) # 300 tagged keywords.
### Gusenbauer & Haddaway (2020) note that search terms of length twenty-five should allow reviewers to
### specify theirsearch scope to a reasonable extent. Put differently, our Boolean search should contain at
### least twenty-five search terms. On the other hand, the maximum search string length that Ovid, Scopus,
### and Web of Science allow according to Gusenbauer & Haddaway (2020) is around 1000. Web of Science for
### example states that the maximum number of terms allowed in one field is fifty terms when using
### "All Fields". As such, this number of keywords is a natural cap to the number of keywords that we can
### seek to obtain from a corpora set. To reiterate, the number of keywords that will be incorporated in the
### final searches will have a lower limit of 25 search terms, and an upper limit of 1000 characters in the
### search string length, which translates to around fifty to sixty search terms (depending on the length of
### the search terms).
tagged_keywords <- litsearchr::extract_terms(
keywords = it1_dedup_out$keywords, # This is a list with the keywords from the articles.
method = "tagged", # Indicate that we want to obtain the keywords from the provided list.
min_freq = 30, # The keyword has to occur a minimum of forty times to be included.
ngrams = TRUE, # 'litsearchr' should only consider an ngram with a minimum of length n,
min_n = 2, # where n is equal to 2.
language = "English") # 'litsearchr' should only consider English keywords.
length(tagged_keywords) # 300 tagged keywords.
### Gusenbauer & Haddaway (2020) note that search terms of length twenty-five should allow reviewers to
### specify theirsearch scope to a reasonable extent. Put differently, our Boolean search should contain at
### least twenty-five search terms. On the other hand, the maximum search string length that Ovid, Scopus,
### and Web of Science allow according to Gusenbauer & Haddaway (2020) is around 1000. Web of Science for
### example states that the maximum number of terms allowed in one field is fifty terms when using
### "All Fields". As such, this number of keywords is a natural cap to the number of keywords that we can
### seek to obtain from a corpora set. To reiterate, the number of keywords that will be incorporated in the
### final searches will have a lower limit of 25 search terms, and an upper limit of 1000 characters in the
### search string length, which translates to around fifty to sixty search terms (depending on the length of
### the search terms).
tagged_keywords <- litsearchr::extract_terms(
keywords = it1_dedup_out$keywords, # This is a list with the keywords from the articles.
method = "tagged", # Indicate that we want to obtain the keywords from the provided list.
min_freq = 40, # The keyword has to occur a minimum of forty times to be included.
ngrams = TRUE, # 'litsearchr' should only consider an ngram with a minimum of length n,
min_n = 2, # where n is equal to 2.
language = "English") # 'litsearchr' should only consider English keywords.
length(tagged_keywords) # 300 tagged keywords.
### Gusenbauer & Haddaway (2020) note that search terms of length twenty-five should allow reviewers to
### specify theirsearch scope to a reasonable extent. Put differently, our Boolean search should contain at
### least twenty-five search terms. On the other hand, the maximum search string length that Ovid, Scopus,
### and Web of Science allow according to Gusenbauer & Haddaway (2020) is around 1000. Web of Science for
### example states that the maximum number of terms allowed in one field is fifty terms when using
### "All Fields". As such, this number of keywords is a natural cap to the number of keywords that we can
### seek to obtain from a corpora set. To reiterate, the number of keywords that will be incorporated in the
### final searches will have a lower limit of 25 search terms, and an upper limit of 1000 characters in the
### search string length, which translates to around fifty to sixty search terms (depending on the length of
### the search terms).
tagged_keywords <- litsearchr::extract_terms(
keywords = it1_dedup_out$keywords, # This is a list with the keywords from the articles.
method = "tagged", # Indicate that we want to obtain the keywords from the provided list.
min_freq = 40, # The keyword has to occur a minimum of forty times to be included.
ngrams = TRUE, # 'litsearchr' should only consider an ngram with a minimum of length n,
min_n = 2, # where n is equal to 2.
language = "English") # 'litsearchr' should only consider English keywords.
length(tagged_keywords) # 300 tagged keywords.
### Now use the RAKE to obtain keywords from the titles and abstracts of the search corpus. We extract all
### non-single keywords that occur at least forty times in these titles and abstracts.
raked_keywords <- litsearchr::extract_terms(
text = paste(it1_dedup_out$title, it1_dedup_out$abstract), # This is a list of titles and abstracts.
method = "fakerake", # Indicate that 'litsearchr' should use the RAKE algorithm.
min_freq = 40, # The keyword has to occur a minimum of forty times to be included.
ngrams = TRUE, # 'litsearchr' should only consider an ngram with a minimum of length n,
min_n = 2, # where n is equal to 2.
language = "English") # 'litsearchr' should only consider English keywords.
length(raked_keywords) # 485 raked keywords.
## Sum total of tagged and raked keywords is 180 + 317 = 497.
keyword_candidates <- remove_redundancies(c(tagged_keywords, raked_keywords), closure = "full") # Remove
# duplicate terms.
length(keyword_candidates) # Total of 640 keyword candidates.
keyword_candidates
## The subsequent task is to select all keywords that are relevant to our query from the candidate pool. I
## select terms that relate in content to the relevant literature, i.e., are a independent or dependent
## variable of interest in the group contact on inter-ethnic attitudes relationship. Note that I interpret
## relevance quite broadly, since these terms will be will be ranked and filtered further based on their
## "connectedness" in a co-occurrence network. In that sense, it is better too be too liberal than too
## selective in our choices here, since it might be hard to anticipate which relevant terms are important
## from this "connectedness" perspective. I furthermore do not include keywords which relate directly to any
## of the other paradigms, e.g., I do not include keywords on group threat theory even though these might
## appear often in the group threat literature. Finally note that I do this part manually, which is prone to
## errors.
all_keywords <- keyword_candidates[
c(2, 3, 13, 15, 17, 18, 22, 27, 29, 32, 35, 36, 37, 38, 39, 41, 44, 45, 46, 47, 48, 49, 50, 62, 64, 65, 69,
70, 72, 73, 74, 75, 78, 79, 83, 94, 95, 96, 101, 102, 105, 109, 111, 116, 118, 120, 121, 122, 124, 126,
127, 134, 135, 137, 142, 144, 147, 149, 150, 151, 152, 153, 154, 161, 164, 167, 168, 173, 188, 201, 202,
215, 216, 218, 222, 223, 224, 225, 226, 227, 228, 235, 250, 251, 256, 259, 260, 264, 266, 267, 269, 270,
271, 275, 276, 277, 278, 281, 282, 283, 284, 285, 288, 289, 290, 292, 293, 297, 308, 309, 310, 311, 318,
320, 323, 324, 325, 329, 330, 331, 336, 351, 353, 354, 359, 360, 361, 382, 387, 388)
]
all_keywords
## Manual cleaning.
all_keywords <- all_keywords[-c(1)] # Remove "0410 group interactions".
all_keywords[1] <- "group interactions" # Re-format.
all_keywords
(all_keywords <- sort(all_keywords))
length(all_keywords) # 186 keyword candidates.
### co-occurrence network (KCN). In a KCN, each keyword is "represented as a node and each co-occurrence of
### a pair of words is represented as a link" (Radhakrishnan, Erbis, Isaacs, & Kamarthi, 2017). "The number
### of times that a pair of words co-occurs in multiple articles constitutes the weight of the link
### connecting the pair" (Radhakrishnan, Erbis, Isaacs, & Kamarthi, 2017). "The network constructed in this
### manner represents cumulative knowledge of a domain and helps to uncover meaningful knowledge components
### and insights based on the patterns and strength of links between keywords that appear in the literature"
### (Radhakrishnan, Erbis, Isaacs, & Kamarthi, 2017). I furthermore combine the second iteration keywords
## with the naive keywords. I add the naive keywords to the keyword candidate selection under the assumption
## that these are important keywords because they were obtained from the gold standard articles, and should
## therefore be considered explicitly in the KCN analysis, even though they are implicitly represented.
all_keywords_final <- c()
all_keywords_final <- append(all_keywords_final, c(as.vector(unlist(gs_grouped_terms)), all_keywords))
all_keywords_final <- remove_redundancies(all_keywords_final, closure = "full") # Remove duplicates.
length(all_keywords_final) # 204 candidates.
## Build the keyword co-occurrence network. This chunk of code is a reworked version of the tutorial at:
## https://luketudge.github.io/litsearchr-tutorial/litsearchr_tutorial.html.
filter_dfm <- litsearchr::create_dfm(
elements = paste(it1_dedup_out$title, it1_dedup_out$abstract), # The input in which the keywords can
# co-occur, titles and abstracts.
features = all_keywords_final) # The keyword candidates.
kcn <- create_network(filter_dfm) # Create a KCN.
## Rank keywords based on strength in the co-occurrence network.
strengths <- strength(kcn) # Calculate strength values.
data.frame(term = names(strengths), strength = strengths, row.names = NULL) %>%
mutate(rank = rank(strength, ties.method = "min")) %>%
arrange(desc(strength)) ->
term_strengths # Create a data frame where the keywords are sorted by strength, in descending order.
## I now apply a filter to select 57 and 42 terms for the search string in OVID, Scopus, and Web of Science
## and ProQuest, respectively, where I remove terms which refer to an equivalent or near equivalent concept,
## i.e., "target group" and "target groups", where I select the term with the highest strength value.
(term_strengths <- cbind(seq(length(term_strengths$term)), term_strengths[order(term_strengths$term), ]))
